Check the sanity of the data.. Run Str(xx), head(xx), tail(xx) etc.
Exclude variables with missing values that start with..Max, min, amplitude, var, stddev, avg, kurtosis, skewness. Use the starts_-with function to so this
Exclude the first 7 variables since they do not seem to be useful in the prediction process and since it has been mentioned in the instruction for this assignment that we could use any of the predictors
Exploratory analysis.
-          Have a look at the frequency of the response variable, "classe". The observation is that there is a fair representation of the different classes in the classe variable in the data. This is done using the table(training_assig3$classe)%>%barplot(col="grey")
The prediction process:
1.       The first step was to fit a classification tree algorithm to the training dataset. This was done  using the tree function given  as.. Class_tree2=tree(classe~.,Train_sets) which could also be run using the train and rpart functions which are given by train(classe~.,method="rpart",data=training_assig3) and  rpart(classe~.,data=training_assig3, method="class") respectively. These methods however gave different rates of accuracy on the test set which is 66%, 50% and 75% respectively.
2.       Next, we use cross validation to prune the trees above to see if we can get improved results. For this the result of the tree function illustrated above is used. Running the summary of the tree ,Summary(class_tree2) shows that we have 18 terminal nodes. Running cross validation on this tree as cv.class_tree2=cv.tree(class_tree2,FUN=prune.misclass) also gives 18 terminal nodes which give the smallest cross validation erro rate. Therefore, we can see that pruning doesn't really improve the results and we do not need to run the pruning function. Cv.tree runs a k-fold cross validation to find the deviance as a function of a cost-complexity parameter. If not specified, cv.tree uses a 10 fold cross validation, which is the case above 
3.        Next we apply "bagging" to the data as cv.class.bag=randomForest(classe~.,data=Train_sets,mtry=52,importance=TRUE)  where mtry=52 means that we are using all the predictors. Applying the prediction to the test set as predict(cv.class.rf,Test_sets)  and calculating the accuracy gives a 98% accuracy
4.       Then we apply randomForest to the training dataset as randomForest(classe~.,data=Train_sets,mtry=8,importance=TRUE) where mtry=8 is an approximate value used for sqrt(number of predictors). Random forest also gives a high accura y rate of 99.4%
5.       Next, I use the machine learning algorithm in 4. above (randomForest) to predict the 20 test cases. Before running the prediction on the test set, I remove all the variables I removed from the training dataset from the test datasets as well. Once i have matching the variables names of the training dataset and test set,   I apply the prediction as predict_20_cases=predict(predict(cv.class.rf,testing_assig3) testing_assig3 being the test dataset with 20 observations. The prediction is as follows in the order given by the test dataset. B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
 
